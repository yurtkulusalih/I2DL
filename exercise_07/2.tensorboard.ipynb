{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Introduction\n",
    "\n",
    "In this tutorial, we’ll learn how to:\n",
    "\n",
    "1. Set up TensorBoard;\n",
    "2. Write something to TensorBoard, e.g. images;\n",
    "3. Inspect a model architecture using TensorBoard;\n",
    "4. Train model and write loss, accuracy and some images to TensorBoard;\n",
    "5. Play with different weight initialization techniques in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Short Introduction about TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "You can download TensorBoard by simply running the following command in terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard==2.0 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorboard\n",
    "print(f\"Tensorboard version: {tensorboard.__version__}\")\n",
    "if not tensorboard.__version__.startswith(\"2.0\"):\n",
    "    print(\"You are using an another version of Tensorboard. We expect Tensorboard 2.0.0. You can continue with your version but it\"\n",
    "          \" might cause some issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that version of your tensorboard module should match your torch version, and you may find all blank after logging into the TensorBoard site if the versions mismatch. If such error occurs, you may try the following command to download tensorboad with specific **VERSION_NUMBER** you want:\n",
    "\n",
    "```pip install tensorboard==VERSION_NUMBER```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1 to 4 in this tutorial are highly aligned with the official tutorial，which is very clear and straighforward, you may also check and play with it in coLab：\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorBoard Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load dataset and define our simple network from the pytorch intro notebook to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])  # mean and std have to be sequences (e.g. tuples),\n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                          download=True, transform=transform)\n",
    "\n",
    "fashion_mnist_test_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                          download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(fashion_mnist_dataset, batch_size=8)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(fashion_mnist_test_dataset, batch_size=8)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.cpu().mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=nn.Sigmoid(),\n",
    "                 input_size=1*28*28, hidden_size=100, classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Here we initialize our activation and set up our two linear layers\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset and setting up the model we want to train, we now import tensorboard from utils and define a **SummaryWriter**, which will be a working station for us later to log and visualize data we want during training. \n",
    "\n",
    "In the following example, we will create a folder called **runs/introduction** to save SummaryWriter and later log into TensorBoard through terminal Command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/introduction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write to TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some stuff to TensorBoard, and log into it to see how things go :)\n",
    "You can log into TensorBoard by running the command from this exercise folder in Terminal:\n",
    "\n",
    "```tensorboard --logdir=runs```\n",
    "\n",
    "For Linux user, you can use open a Terminal and simply run it\n",
    "\n",
    "For Windows user with Anaconda, you may open an Anaconda Prompt and then run the command. Otherwise use your default setup of running python code in cmd.\n",
    "\n",
    "Note that before running the command **you may get into root folder of this Notebook first**. Finally from the command line and then navigating to http://localhost:6006. If everything went well you will be presented with the tensorboard stup and after executing the next cell you should see the following images in TensorBoard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorBoard Interface](./images/imgvis.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to hit the refresh button on the top right as tensorboard will only update in intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture visualization in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the architecture of your model in Tensorboard. Let's see how the input and output dimensions of your model look like TensorBoard. It is also a good way to debug as your model growing more and more complex. You may see the following output after adding the graph of network architecture to the TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Architecture Visualization](./images/net_graph.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net.cpu(), images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the 'GRAPHS' section in the top ribbon to access it and play a bit. The above image was generated by clicking on our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore one of the most important use of TensorBoard in model training, that is, tracking the loss and accuracy every some iterations. Now, we’ll instead log the running loss to TensorBoard, along with a view into the predictions the model is making via the plot_classes_preds function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s start training the model and writing results to TensorBoard every 1000 batches instead of printing to console; this is done using the **add_scalar** function. \n",
    "\n",
    "In addition, as we train, we’ll generate an image showing the model’s predictions vs. the actual results on the four images included in that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "running_loss = 0.0\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "            print(\"[Epoch %d, Iteration %5d]\" % (epoch+1, i+1))\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('Training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('Predictions vs Actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            i)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a printing output here in the notebook, you will see the current loss log in Tensorboard under 'SCALARS' in more or less real time. We also generated a prediction image for you under 'IMAGES'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Weight Initialization Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be already familier with some basic writing functions in TensorBoard. Let's explore the effect weight initialization using TensorBoard, and here we will also use **add_historgram** function to visualize the output distribution of each layer.\n",
    "\n",
    "In previous exercises we used a naive Gaussian initialization, though in the lectures you learned that one needs to be careful about the type of weight initialization. In addition, it is reliant on your activation function of choice. Let's replicate those experiments!\n",
    "\n",
    "We write all important information to Tensorboard, so check your results there after every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/weighit_init_experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a dummy network and keep track of the output of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation_method):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.x1 = torch.Tensor([])\n",
    "        self.x2 = torch.Tensor([])\n",
    "        self.x3 = torch.Tensor([])\n",
    "        self.x4 = torch.Tensor([])\n",
    "        self.x5 = torch.Tensor([])\n",
    "        self.x6 = torch.Tensor([])\n",
    "                \n",
    "        self.fc1 = nn.Linear(28*28, 300)\n",
    "        self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc3 = nn.Linear(300, 300)\n",
    "        self.fc4 = nn.Linear(300, 300)\n",
    "        self.fc5 = nn.Linear(300, 300)\n",
    "        self.fc6 = nn.Linear(300, 300)\n",
    "        self.fc7 = nn.Linear(300, 10)\n",
    "        \n",
    "        if activation_method == \"relu\" :\n",
    "            self.activation = nn.ReLU() \n",
    "        elif activation_method == \"tanh\":\n",
    "            self.activation = nn.Tanh() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28*28)\n",
    "        self.x1 = self.activation(self.fc1(x))\n",
    "        self.x2 = self.activation(self.fc2(self.x1))\n",
    "        self.x3 = self.activation(self.fc3(self.x2))\n",
    "        self.x4 = self.activation(self.fc4(self.x3))\n",
    "        self.x5 = self.activation(self.fc5(self.x4))\n",
    "        self.x6 = self.activation(self.fc6(self.x5))\n",
    "        logits = self.fc7 (self.x6)\n",
    "        return logits\n",
    "\n",
    "    def collect_layer_out (self):\n",
    "        return [self.x1, self.x2, self.x3, self.x4, self.x5, self.x6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "visloader = torch.utils.data.DataLoader(fashion_mnist_dataset, batch_size=40, shuffle=True)\n",
    "dataiter = iter(visloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\"tanh\")\n",
    "print(images.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Constant weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with constant weight initialization, what problems do you observe with the distribution of the output of each layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_const = Net(\"tanh\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.constant_(m.weight,2.0)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_const.apply(init_weights)\n",
    "outputs = net_const(images)\n",
    "layer_out = net_const.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('constant_init', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Small random weight initialization with Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last example we can see that initialization with constant have problems of weight symmetry, i.e. all parameters in network always learn the same things. So now we will try some random weight initialization and let's see what happens if weights are initialized with high values or very low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_small_normal = Net(\"tanh\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight,mean=0.0, std=0.01)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_small_normal.apply(init_weights)\n",
    "outputs = net_small_normal(images)\n",
    "layer_out = net_small_normal.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('small_normal_tanh', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Large random weight initialization with Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_large_normal = Net(\"tanh\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight,mean=0.0, std=0.2)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_large_normal.apply(init_weights)\n",
    "outputs = net_large_normal(images)\n",
    "layer_out = net_large_normal.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('large_normal_tanh', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last two examples, we can see that random weight initialization with normal distribution might work well in some shallow layers of the network, while if we are going deeper into the network, it will end up with **vanishing gradient problem**, i.e.\n",
    "\n",
    "1) If weights are initialized with very high values the term $Xw+b$ becomes significantly higher and if an activation function like Tanh is applied, the function maps its value near to -1 or 1, where the gradient is very low and learning takes a lot of time.\n",
    "\n",
    "2) If weights are initialized with low values it gets mapped to around 0,and the rather small values will kill gradients when doing backpropagation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Xavier initialization with Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous examples, we can see that a proper weight initialization is a method that will ensure nice distribution of the ouput of each layer. As a result, we may turn to **Xavier Initialization**:\n",
    "\n",
    "We will fill the weight with values using a normal distribution $\\mathcal{N}(0,std^2)$ where\n",
    "\n",
    "$$ std = gain \\times \\sqrt{\\frac{2}{fan _{in} + fan_{out}}} $$\n",
    "\n",
    "Note here $fan _{in}$ and $ fan_{out} $ are dimensions of the ipunt and output layer and $gain$ is a optional scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xavier = Net(\"tanh\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_xavier.apply(init_weights)\n",
    "outputs = net_xavier(images)\n",
    "layer_out = net_xavier.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('xavier_tanh', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Xavier initialization with ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a zero centered activation function in xavier activation is assumed, and we can see here activations collapse to zero again if we use ReLU as activation function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xavier_relu = Net(\"relu\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_xavier_relu.apply(init_weights)\n",
    "outputs = net_xavier_relu(images)\n",
    "layer_out = net_xavier_relu.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('xavier_relu', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 He initialization with ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we may turn to **He Initialization** to resolve the problems of non-centered activation functions.  \n",
    "We will fill the weight with values using a normal distribution $\\mathcal{N}(0,std^2)$ where\n",
    "\n",
    "$$ std = \\frac {gain} {\\sqrt{fan_{mode}}} $$\n",
    "\n",
    "Note here $fan _{mode}$ can be chosen either $fan _{in}$ (default) or $fan _{out}$. Choosing $fan _{in}$ preserves the magnitude of the variance of the weights in the forward pass. Choosing $fan _{out}$ preserves the magnitudes in the backwards pass. and $gain$ is also a optional scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_kaiming_relu = Net(\"relu\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_uniform_(m.weight,nonlinearity='relu')\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "net_kaiming_relu.apply(init_weights)\n",
    "outputs = net_kaiming_relu(images)\n",
    "layer_out = net_kaiming_relu.collect_layer_out()\n",
    "\n",
    "for i, x in enumerate(layer_out):\n",
    "    writer.add_histogram('kaiming_relu', x, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, you should have everything at hand to work with Tensorboard. It is highly advised to use either Tensorboard or other similar libraries, such as visdom, instead of implementing everything yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
